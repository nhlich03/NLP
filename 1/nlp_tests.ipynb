{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\ThienLaptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ThienLaptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\ThienLaptop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('the', 'DT'), ('party', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"We are going to the party\")\n",
    "print(nltk.pos_tag(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.generate import generate, demo_grammar\n",
    "from nltk import CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  S -> NP VP\n",
      "  NP -> Det N\n",
      "  PP -> P NP\n",
      "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
      "  Det -> 'the' | 'a'\n",
      "  N -> 'man' | 'park' | 'dog'\n",
      "  P -> 'in' | 'with'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(demo_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the first 23 sentences for demo grammar:\n",
      "\n",
      "  S -> NP VP\n",
      "  NP -> Det N\n",
      "  PP -> P NP\n",
      "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
      "  Det -> 'the' | 'a'\n",
      "  N -> 'man' | 'park' | 'dog'\n",
      "  P -> 'in' | 'with'\n",
      "\n",
      "  1. the man slept\n",
      "  2. the man saw the man\n",
      "  3. the man saw the park\n",
      "  4. the man saw the dog\n",
      "  5. the man saw a man\n",
      "  6. the man saw a park\n",
      "  7. the man saw a dog\n",
      "  8. the man walked in the man\n",
      "  9. the man walked in the park\n",
      " 10. the man walked in the dog\n",
      " 11. the man walked in a man\n",
      " 12. the man walked in a park\n",
      " 13. the man walked in a dog\n",
      " 14. the man walked with the man\n",
      " 15. the man walked with the park\n",
      " 16. the man walked with the dog\n",
      " 17. the man walked with a man\n",
      " 18. the man walked with a park\n",
      " 19. the man walked with a dog\n",
      " 20. the park slept\n",
      " 21. the park saw the man\n",
      " 22. the park saw the park\n",
      " 23. the park saw the dog\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import itertools\n",
    "import sys\n",
    "from nltk.grammar import Nonterminal\n",
    "\n",
    "\n",
    "def generate(grammar, start=None, depth=None, n=None):\n",
    "    \"\"\"\n",
    "    Generates an iterator of all sentences from a CFG.\n",
    "\n",
    "    :param grammar: The Grammar used to generate sentences.\n",
    "    :param start: The Nonterminal from which to start generate sentences.\n",
    "    :param depth: The maximal depth of the generated tree.\n",
    "    :param n: The maximum number of sentences to return.\n",
    "    :return: An iterator of lists of terminal tokens.\n",
    "    \"\"\"\n",
    "    if not start:\n",
    "        start = grammar.start()\n",
    "    if depth is None:\n",
    "        depth = sys.maxsize\n",
    "\n",
    "    iter = _generate_all(grammar, [start], depth)\n",
    "\n",
    "    if n:\n",
    "        iter = itertools.islice(iter, n)\n",
    "\n",
    "    return iter\n",
    "\n",
    "\n",
    "\n",
    "def _generate_all(grammar, items, depth):\n",
    "    if items:\n",
    "        try:\n",
    "            for frag1 in _generate_one(grammar, items[0], depth):\n",
    "                for frag2 in _generate_all(grammar, items[1:], depth):\n",
    "                    yield frag1 + frag2\n",
    "        except RuntimeError as _error:\n",
    "            if _error.message == \"maximum recursion depth exceeded\":\n",
    "                # Helpful error message while still showing the recursion stack.\n",
    "                raise RuntimeError(\"The grammar has rule(s) that yield infinite recursion!!\")\n",
    "            else:\n",
    "                raise\n",
    "    else:\n",
    "        yield []\n",
    "\n",
    "\n",
    "def _generate_one(grammar, item, depth):\n",
    "    if depth > 0:\n",
    "        if isinstance(item, Nonterminal):\n",
    "            for prod in grammar.productions(lhs=item):\n",
    "                for frag in _generate_all(grammar, prod.rhs(), depth-1):\n",
    "                    yield frag\n",
    "        else:\n",
    "            yield [item]\n",
    "\n",
    "demo_grammar = \"\"\"\n",
    "  S -> NP VP\n",
    "  NP -> Det N\n",
    "  PP -> P NP\n",
    "  VP -> 'slept' | 'saw' NP | 'walked' PP\n",
    "  Det -> 'the' | 'a'\n",
    "  N -> 'man' | 'park' | 'dog'\n",
    "  P -> 'in' | 'with'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def demo(N=23):\n",
    "    from nltk.grammar import CFG\n",
    "\n",
    "    print('Generating the first %d sentences for demo grammar:' % (N,))\n",
    "    print(demo_grammar)\n",
    "    grammar = CFG.fromstring(demo_grammar)\n",
    "    for n, sent in enumerate(generate(grammar, n=N), 1):\n",
    "        print('%3d. %s' % (n, ' '.join(sent)))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "prepchoices = nltk.ConditionalFreqDist((v[0], p[0]) \n",
    "    for (v, p) in nltk.bigrams(brown.tagged_words(tagset=\"universal\")) \n",
    "        if v[1] == \"VERB\" and p[1] == \"ADP\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'in': 5, 'from': 3, 'at': 3, 'to': 2, 'about': 1, 'under': 1, 'with': 1, 'on': 1, 'for': 1, 'since': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepchoices[\"writing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {}\n",
    "grammar[\"sitting\"] = {}\n",
    "grammar[\"sitting\"][\"table\"] = \"on\"\n",
    "grammar[\"sitting\"][\"van\"] = \"in\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sitting': {'table': 'on', 'van': 'in'}}\n"
     ]
    }
   ],
   "source": [
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bell, saurav]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "sent = \"when the bell rang, saurav went out\"\n",
    "doc=nlp(sent)\n",
    "\n",
    "sub_toks = [tok for tok in doc if (tok.dep_ == \"nsubj\") ]\n",
    "\n",
    "print(sub_toks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'spacy.tokens.span.Span' object has no attribute 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(raw_text)\n\u001b[1;32m----> 6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [sent\u001b[38;5;241m.\u001b[39mstring\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentences)\n",
      "Cell \u001b[1;32mIn [23], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(raw_text)\n\u001b[1;32m----> 6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[43msent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstring\u001b[49m\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentences)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'spacy.tokens.span.Span' object has no attribute 'string'"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "raw_text = 'Hello, world. Here are two sentences.'\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(raw_text)\n",
    "sentences = [sent.string.strip() for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "doc = nlp(u\"the shop is closed.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, token.lemma, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noun_chunks(doc, drop_determiners=True, min_freq=1):\n",
    "    \"\"\"\n",
    "    Extract an ordered sequence of noun chunks from a spacy-parsed doc, optionally\n",
    "    filtering by frequency and dropping leading determiners.\n",
    "    Args:\n",
    "        doc (``textacy.Doc`` or ``spacy.Doc``)\n",
    "        drop_determiners (bool): remove leading determiners (e.g. \"the\")\n",
    "            from phrases (e.g. \"the quick brown fox\" => \"quick brown fox\")\n",
    "        min_freq (int): remove chunks that occur in ``doc`` fewer than\n",
    "            ``min_freq`` times\n",
    "    Yields:\n",
    "        ``spacy.Span``: the next noun chunk from ``doc`` in order of appearance\n",
    "        in the document\n",
    "    \"\"\"\n",
    "    if hasattr(doc, 'spacy_doc'):\n",
    "        ncs = doc.spacy_doc.noun_chunks\n",
    "    else:\n",
    "        ncs = doc.noun_chunks\n",
    "    if drop_determiners is True:\n",
    "        ncs = (nc if nc[0].pos != DET else nc[1:]\n",
    "               for nc in ncs)\n",
    "    if min_freq > 1:\n",
    "        ncs = list(ncs)\n",
    "        freqs = itertoolz.frequencies(nc.lower_ for nc in ncs)\n",
    "        ncs = (nc for nc in ncs\n",
    "               if freqs[nc.lower_] >= min_freq)\n",
    "\n",
    "    for nc in ncs:\n",
    "        yield nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_chunks(\"the boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "\n",
    "doc = en_nlp(\"The downside is that, because statistical programs are easy to use, it is equally easy to do the wrong analysis.\")\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "\n",
    "doc = en_nlp(\"children plays in the garden\")\n",
    "\n",
    "def tok_format(tok):\n",
    "    return \"_\".join([tok.orth_, tok.dep_, tok.tag_])\n",
    "\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(tok_format(node), [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return tok_format(node)\n",
    "\n",
    "\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = en_nlp(\"He was swimming in the river\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = en_nlp(\"When he got the email, he came to my small office house and started shouting.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = en_nlp(\"I am walking on the road.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = en_nlp(\"The little boys were playing in the garden\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = en_nlp(\"Admist all confusion, Salman was found guilty in the case.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "for sent in doc3.sents:\n",
    "    print(sent.root)\n",
    "    for ch in sent.root.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            print(ch)\n",
    "            for sec in ch.children:\n",
    "                print(sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = en_nlp(\"The mother was cooking dinner in the home kitchen and the boys were playing in the garden.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc3.sents]\n",
    "\n",
    "grammar = {}\n",
    "\n",
    "def VB_IN_NN(payload):\n",
    "    if(payload.tag_[:2] != 'VB'):\n",
    "        return\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            VB_IN_NN(ch)\n",
    "    temp = [payload]\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_ == \"IN\"):\n",
    "            temp.append(ch)\n",
    "            for sec in ch.children:\n",
    "                temp.append(sec)\n",
    "                if(len(temp) == 3):\n",
    "                    grammar[payload.text.lower()] = {}\n",
    "                    grammar[payload.text.lower()][sec.text.lower()] = ch.text.lower()\n",
    "                return\n",
    "    \n",
    "\n",
    "for sent in doc3.sents:\n",
    "    VB_IN_NN(sent.root)\n",
    "print(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "mdetok = MosesDetokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in brown.sents('cb01')[:20]:\n",
    "    munged_sentence = ' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\")\n",
    "    print(mdetok.detokenize(munged_sentence.split(), return_str=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "for cps in brown.fileids()[:10]:\n",
    "    \n",
    "    for sent in brown.sents(cps):\n",
    "        count += 1\n",
    "        munged_sentence = ' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\")\n",
    "        doc4 = en_nlp(mdetok.detokenize(munged_sentence.split(), return_str=True))\n",
    "        #[to_nltk_tree(sent.root).pretty_print() for sent in doc4.sents]\n",
    "        for sent in doc4.sents:\n",
    "            VB_IN_NN(sent.root)\n",
    "\n",
    "print(grammar)\n",
    "#print(str(len(combos)) + \" \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar[\"joined\"][\"1925\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('correctly.npy', grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_grammar = np.load('correctly.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_IN_NN_correction(payload, raw_text, master_dictionary):\n",
    "\tif(payload.tag_[:2] != 'VB'):\n",
    "\t\treturn\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_[:2] == 'VB'):\n",
    "\t\t\tVB_IN_NN(ch)\n",
    "\ttemp = [payload]\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_ == \"IN\"):\n",
    "\t\t\ttemp.append(ch)\n",
    "\t\t\tfor sec in ch.children:\n",
    "\t\t\t\ttemp.append(sec)\n",
    "\t\t\t\tif(len(temp) == 3):\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tcorrect_prep = master_dictionary[payload.text.lower()][sec.text.lower()]\n",
    "\t\t\t\t\t\tif(correct_prep != ch.text.lower()):\n",
    "\t\t\t\t\t\t\traw_text = raw_text[:ch.idx] + raw_text[ch.idx:].replace(temp[1].text, correct_prep, 1)\n",
    "\t\t\t\t\t\t\treturn raw_text\n",
    "\t\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\t\treturn raw_text\n",
    "\t\t\t\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i was dancing with the park.\"\n",
    "doc = en_nlp(text)\n",
    "for sent in doc.sents:\n",
    "    text = VB_IN_NN_correction(sent.root, text, grammar)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import conjugate, lemma, lexeme, INFINITIVE, PRESENT, PAST, PARTICIPLE, FUTURE, SG, PL, INDICATIVE, IMPERATIVE, CONDITIONAL, SUBJUNCTIVE, PROGRESSIVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conjugate(verb='downloading', tense=PRESENT, mood=INDICATIVE, aspect=PROGRESSIVE, person=1, number=PL)) # add aspect=PROGRESSIVE to indicate continuous tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = en_nlp(\"has ram taken the ball?\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = en_nlp(\"ram has been watching tv.\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc2.sents:\n",
    "    for comp in sent.root.children:\n",
    "        if(comp.tag_ == 'VBD'):\n",
    "            print(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_VB_VB(payload):\n",
    "    if(payload.tag_[:2] != 'VB'):\n",
    "        return\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            VB_VB_VB(ch)\n",
    "    temp = []\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            temp.append(ch.lower_ + '_' + ch.tag_)\n",
    "        if(len(temp) == 2):\n",
    "            temp.append(payload.lower_+ '_' + ch.tag_)\n",
    "            combos.append(temp)\n",
    "            temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "rtext = \"Ramu has been travel since early this year.\"\n",
    "doc2 = en_nlp(rtext)\n",
    "combos = []\n",
    "for sent in doc2.sents:\n",
    "    rtext = VB_VB_VB_correction(sent.root, rtext)\n",
    "print(rtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_VB_VB_correction(payload, raw_text):\n",
    "    if(payload.tag_[:2] != 'VB' and payload.tag_[:2] != 'NN'  and payload.tag_[:2] != 'JJ'):\n",
    "        return\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'): # this might need to be removed\n",
    "            VB_VB_VB_correction(ch, raw_text)\n",
    "    temp = []\n",
    "    nounBeforeVerb = False\n",
    "    nounAfterVerb = False\n",
    "    verbFound = False\n",
    "    since = False\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            verbFound = True\n",
    "        if((not verbFound) and (ch.tag_[:2] == 'NN' or ch.tag_[:2] == 'PR')):\n",
    "            nounBeforeVerb = True\n",
    "        if(verbFound and (ch.tag_[:2] == 'NN' or ch.tag_[:2] == 'PR')):\n",
    "            nounAfterVerb = True\n",
    "        if(ch.lower_ == 'since'):\n",
    "            since = True\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'):\n",
    "            # print(ch.idx)\n",
    "            temp.append(ch.lower_ + '_' + ch.tag_)\n",
    "        if(len(temp) == 2):\n",
    "            temp.append(payload.lower_+ '_' + ch.tag_)\n",
    "            #print(temp)\n",
    "            if (temp[0][-3:] == 'VBZ' or temp[0][-3:] == 'VBP') and temp[1][-3:] == 'VBN':\n",
    "                if nounAfterVerb or since:\n",
    "                    x = conjugate(verb=lemma(temp[2][:-4]), tense=PRESENT, mood=INDICATIVE, aspect=PROGRESSIVE, person=1, number=PL)\n",
    "                elif nounBeforeVerb:\n",
    "                    x = conjugate(verb=lemma(temp[2][:-4]), tense=PAST+PARTICIPLE, mood=INDICATIVE, person=1, number=PL)\n",
    "                # print(temp[2][:-4] + ' -> ' + x)\n",
    "            combos.append(temp)\n",
    "            # print(nounBeforeVerb)\n",
    "            raw_text = raw_text[:payload.idx] + raw_text[payload.idx:].replace(temp[2][:-4], x, 1)\n",
    "            #print(raw_text)\n",
    "            temp = []\n",
    "            return raw_text\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_IN_NN(payload):\n",
    "\tif(payload.tag_[:2] != 'VB'):\n",
    "\t\treturn\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_[:2] == 'VB'):\n",
    "\t\t\tVB_IN_NN(ch)\n",
    "\ttemp = [payload]\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_ == \"IN\"):\n",
    "\t\t\ttemp.append(ch)\n",
    "\t\t\tfor sec in ch.children:\n",
    "\t\t\t\ttemp.append(sec)\n",
    "\t\t\t\tif(len(temp) == 3):\n",
    "\t\t\t\t\tgrammar[payload.text.lower()] = {}\n",
    "\t\t\t\t\tgrammar[payload.text.lower()][sec.text.lower()] = ch.text.lower()\n",
    "\t\t\t\treturn\n",
    "            \n",
    "def VB_IN_NN_correction(payload, raw_text, master_dictionary):\n",
    "\tif(payload.tag_[:2] != 'VB'):\n",
    "\t\treturn\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_[:2] == 'VB'):\n",
    "\t\t\tVB_IN_NN_correction(ch, raw_text, master_dictionary)\n",
    "\ttemp = [payload]\n",
    "\tfor ch in payload.children:\n",
    "\t\tif(ch.tag_ == \"IN\"):\n",
    "\t\t\ttemp.append(ch)\n",
    "\t\t\tfor sec in ch.children:\n",
    "\t\t\t\ttemp.append(sec)\n",
    "\t\t\t\tif(len(temp) == 3):\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\tcorrect_prep = master_dictionary[payload.text.lower()][sec.text.lower()]\n",
    "\t\t\t\t\t\tif(correct_prep != ch.text.lower()):\n",
    "\t\t\t\t\t\t\traw_text = raw_text[:ch.idx] + raw_text[ch.idx:].replace(temp[1].text, correct_prep, 1)\n",
    "\t\t\t\t\t\t\treturn raw_text\n",
    "\t\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\t\treturn raw_text\n",
    "\treturn raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VB_VB_correction(payload, raw_text):\n",
    "    if(payload.tag_[:2] != 'VB'):\n",
    "        return\n",
    "    for ch in payload.children:\n",
    "        if(ch.tag_[:2] == 'VB'): # this might need to be removed\n",
    "            VB_VB_VB_correction(ch, raw_text)\n",
    "            \n",
    "            if(ch.lower_ == 'has') or (ch.lower_ == 'have') or (ch.lower_ == 'had'):\n",
    "                x = conjugate(verb=lemma(payload.text), tense=PAST+PARTICIPLE, mood=INDICATIVE, person=1, number=PL)\n",
    "            else:\n",
    "                x = conjugate(verb=lemma(payload.text), tense=PRESENT, mood=INDICATIVE, aspect=PROGRESSIVE, person=1, number=PL)\n",
    "        \n",
    "            raw_text = raw_text[:payload.idx] + raw_text[payload.idx:].replace(payload.text, x, 1)\n",
    "            return raw_text\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = en_nlp(\"he has doing his homework\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = en_nlp(\"he is walking on the road\")\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc2.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "en_nlp = spacy.load('en')\n",
    "rtext = \"He has done his homework.\"\n",
    "doc2 = en_nlp(rtext)\n",
    "combos = []\n",
    "for sent in doc2.sents:\n",
    "    rtext = VB_VB_correction(sent.root, rtext)\n",
    "print(rtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
