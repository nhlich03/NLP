{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I am looking forward to have a dinner with Saurav today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I - nsubj\n",
      "am - aux\n",
      "looking - ROOT\n",
      "forward - advmod\n",
      "to - aux\n",
      "have - advcl\n",
      "a - det\n",
      "dinner - dobj\n",
      "with - prep\n",
      "Saurav - pobj\n",
      "today - npadvmod\n",
      ". - punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text + ' - ' + token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I nsubj looking VERB []\n",
      "am aux looking VERB []\n",
      "looking ROOT looking VERB [I, am, forward, have, .]\n",
      "forward advmod looking VERB []\n",
      "to aux have VERB []\n",
      "have advcl looking VERB [to, dinner, today]\n",
      "a det dinner NOUN []\n",
      "dinner dobj have VERB [a, with]\n",
      "with prep dinner NOUN [Saurav]\n",
      "Saurav pobj with ADP []\n",
      "today npadvmod have VERB []\n",
      ". punct looking VERB []\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "          [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n",
      "Vectorization...\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 4, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 12s 272us/step - loss: 1.8892 - acc: 0.3212 - val_loss: 1.7918 - val_acc: 0.3436\n",
      "Q 580+417 T 997  \u001b[91m☒\u001b[0m 108 \n",
      "Q 22+96   T 118  \u001b[91m☒\u001b[0m 12  \n",
      "Q 216+525 T 741  \u001b[91m☒\u001b[0m 108 \n",
      "Q 53+940  T 993  \u001b[91m☒\u001b[0m 108 \n",
      "Q 48+534  T 582  \u001b[91m☒\u001b[0m 108 \n",
      "Q 803+26  T 829  \u001b[91m☒\u001b[0m 108 \n",
      "Q 678+429 T 1107 \u001b[91m☒\u001b[0m 108 \n",
      "Q 165+919 T 1084 \u001b[91m☒\u001b[0m 108 \n",
      "Q 449+74  T 523  \u001b[91m☒\u001b[0m 108 \n",
      "Q 45+831  T 876  \u001b[91m☒\u001b[0m 101 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 12s 257us/step - loss: 1.7354 - acc: 0.3591 - val_loss: 1.6542 - val_acc: 0.3810\n",
      "Q 967+77  T 1044 \u001b[91m☒\u001b[0m 106 \n",
      "Q 539+511 T 1050 \u001b[91m☒\u001b[0m 102 \n",
      "Q 5+103   T 108  \u001b[91m☒\u001b[0m 166 \n",
      "Q 626+857 T 1483 \u001b[91m☒\u001b[0m 1667\n",
      "Q 71+677  T 748  \u001b[91m☒\u001b[0m 777 \n",
      "Q 760+4   T 764  \u001b[91m☒\u001b[0m 666 \n",
      "Q 796+0   T 796  \u001b[91m☒\u001b[0m 166 \n",
      "Q 69+31   T 100  \u001b[91m☒\u001b[0m 16  \n",
      "Q 3+641   T 644  \u001b[91m☒\u001b[0m 666 \n",
      "Q 22+190  T 212  \u001b[91m☒\u001b[0m 226 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 13s 285us/step - loss: 1.5784 - acc: 0.4080 - val_loss: 1.4991 - val_acc: 0.4330\n",
      "Q 747+12  T 759  \u001b[91m☒\u001b[0m 774 \n",
      "Q 25+7    T 32   \u001b[91m☒\u001b[0m 10  \n",
      "Q 225+198 T 423  \u001b[91m☒\u001b[0m 594 \n",
      "Q 961+755 T 1716 \u001b[91m☒\u001b[0m 1774\n",
      "Q 907+78  T 985  \u001b[91m☒\u001b[0m 904 \n",
      "Q 960+694 T 1654 \u001b[91m☒\u001b[0m 1674\n",
      "Q 74+13   T 87   \u001b[91m☒\u001b[0m 13  \n",
      "Q 438+159 T 597  \u001b[91m☒\u001b[0m 544 \n",
      "Q 435+479 T 914  \u001b[91m☒\u001b[0m 104 \n",
      "Q 391+530 T 921  \u001b[91m☒\u001b[0m 104 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 12s 258us/step - loss: 1.4182 - acc: 0.4706 - val_loss: 1.3671 - val_acc: 0.4777\n",
      "Q 623+300 T 923  \u001b[91m☒\u001b[0m 866 \n",
      "Q 3+982   T 985  \u001b[91m☒\u001b[0m 986 \n",
      "Q 758+77  T 835  \u001b[91m☒\u001b[0m 836 \n",
      "Q 589+42  T 631  \u001b[91m☒\u001b[0m 508 \n",
      "Q 413+33  T 446  \u001b[92m☑\u001b[0m 446 \n",
      "Q 950+823 T 1773 \u001b[91m☒\u001b[0m 1639\n",
      "Q 992+737 T 1729 \u001b[91m☒\u001b[0m 1657\n",
      "Q 216+525 T 741  \u001b[91m☒\u001b[0m 666 \n",
      "Q 39+934  T 973  \u001b[91m☒\u001b[0m 906 \n",
      "Q 54+90   T 144  \u001b[91m☒\u001b[0m 147 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 248us/step - loss: 1.2696 - acc: 0.5290 - val_loss: 1.2182 - val_acc: 0.5504\n",
      "Q 4+92    T 96   \u001b[91m☒\u001b[0m 91  \n",
      "Q 859+68  T 927  \u001b[91m☒\u001b[0m 956 \n",
      "Q 39+863  T 902  \u001b[91m☒\u001b[0m 966 \n",
      "Q 190+92  T 282  \u001b[91m☒\u001b[0m 299 \n",
      "Q 2+467   T 469  \u001b[91m☒\u001b[0m 461 \n",
      "Q 414+808 T 1222 \u001b[91m☒\u001b[0m 1221\n",
      "Q 56+812  T 868  \u001b[91m☒\u001b[0m 866 \n",
      "Q 885+994 T 1879 \u001b[91m☒\u001b[0m 1855\n",
      "Q 926+496 T 1422 \u001b[91m☒\u001b[0m 1431\n",
      "Q 842+12  T 854  \u001b[91m☒\u001b[0m 856 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 252us/step - loss: 1.1541 - acc: 0.5741 - val_loss: 1.1087 - val_acc: 0.5886\n",
      "Q 56+126  T 182  \u001b[91m☒\u001b[0m 187 \n",
      "Q 930+52  T 982  \u001b[91m☒\u001b[0m 977 \n",
      "Q 717+7   T 724  \u001b[91m☒\u001b[0m 729 \n",
      "Q 945+7   T 952  \u001b[91m☒\u001b[0m 959 \n",
      "Q 470+71  T 541  \u001b[92m☑\u001b[0m 541 \n",
      "Q 20+633  T 653  \u001b[91m☒\u001b[0m 646 \n",
      "Q 580+34  T 614  \u001b[91m☒\u001b[0m 612 \n",
      "Q 84+408  T 492  \u001b[91m☒\u001b[0m 482 \n",
      "Q 66+674  T 740  \u001b[91m☒\u001b[0m 731 \n",
      "Q 7+468   T 475  \u001b[91m☒\u001b[0m 471 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 247us/step - loss: 1.0548 - acc: 0.6129 - val_loss: 1.0406 - val_acc: 0.6099\n",
      "Q 952+71  T 1023 \u001b[91m☒\u001b[0m 1020\n",
      "Q 55+446  T 501  \u001b[91m☒\u001b[0m 500 \n",
      "Q 44+662  T 706  \u001b[91m☒\u001b[0m 710 \n",
      "Q 127+19  T 146  \u001b[91m☒\u001b[0m 140 \n",
      "Q 949+79  T 1028 \u001b[91m☒\u001b[0m 1020\n",
      "Q 33+62   T 95   \u001b[91m☒\u001b[0m 10  \n",
      "Q 78+377  T 455  \u001b[91m☒\u001b[0m 454 \n",
      "Q 728+308 T 1036 \u001b[91m☒\u001b[0m 100 \n",
      "Q 24+628  T 652  \u001b[91m☒\u001b[0m 650 \n",
      "Q 70+922  T 992  \u001b[91m☒\u001b[0m 980 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 249us/step - loss: 0.9736 - acc: 0.6475 - val_loss: 0.9481 - val_acc: 0.6593\n",
      "Q 352+759 T 1111 \u001b[91m☒\u001b[0m 1110\n",
      "Q 20+94   T 114  \u001b[91m☒\u001b[0m 112 \n",
      "Q 378+48  T 426  \u001b[91m☒\u001b[0m 429 \n",
      "Q 368+834 T 1202 \u001b[91m☒\u001b[0m 1209\n",
      "Q 419+263 T 682  \u001b[91m☒\u001b[0m 696 \n",
      "Q 358+257 T 615  \u001b[91m☒\u001b[0m 619 \n",
      "Q 8+277   T 285  \u001b[91m☒\u001b[0m 289 \n",
      "Q 415+5   T 420  \u001b[91m☒\u001b[0m 419 \n",
      "Q 496+300 T 796  \u001b[91m☒\u001b[0m 799 \n",
      "Q 221+9   T 230  \u001b[91m☒\u001b[0m 221 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 11s 247us/step - loss: 0.9007 - acc: 0.6767 - val_loss: 0.8922 - val_acc: 0.6713\n",
      "Q 57+268  T 325  \u001b[91m☒\u001b[0m 322 \n",
      "Q 733+632 T 1365 \u001b[91m☒\u001b[0m 1355\n",
      "Q 945+7   T 952  \u001b[92m☑\u001b[0m 952 \n",
      "Q 142+566 T 708  \u001b[91m☒\u001b[0m 610 \n",
      "Q 505+383 T 888  \u001b[91m☒\u001b[0m 887 \n",
      "Q 9+186   T 195  \u001b[91m☒\u001b[0m 199 \n",
      "Q 34+605  T 639  \u001b[91m☒\u001b[0m 631 \n",
      "Q 535+3   T 538  \u001b[92m☑\u001b[0m 538 \n",
      "Q 758+707 T 1465 \u001b[91m☒\u001b[0m 1477\n",
      "Q 308+65  T 373  \u001b[91m☒\u001b[0m 374 \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''An implementation of sequence to sequence learning for performing addition\n",
    "\n",
    "Input: \"535+61\"\n",
    "Output: \"596\"\n",
    "Padding is handled by using a repeated sentinel character (space)\n",
    "\n",
    "Input may optionally be reversed, shown to increase performance in many tasks in:\n",
    "\"Learning to Execute\"\n",
    "http://arxiv.org/abs/1410.4615\n",
    "and\n",
    "\"Sequence to Sequence Learning with Neural Networks\"\n",
    "http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n",
    "Theoretically it introduces shorter term dependencies between source and target.\n",
    "\n",
    "Two digits reversed:\n",
    "+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n",
    "\n",
    "Three digits reversed:\n",
    "+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n",
    "\n",
    "Four digits reversed:\n",
    "+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n",
    "\n",
    "Five digits reversed:\n",
    "+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "\n",
    "\n",
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 50000\n",
    "DIGITS = 3\n",
    "REVERSE = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789+ '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    a, b = f(), f()\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "# Try replacing GRU, or SimpleRNN.\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
    "# Note: In a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, num_feature).\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "# As the decoder RNN's input, repeatedly provide with the last hidden state of\n",
    "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
    "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, return not only the last output but\n",
    "    # all the outputs so far in the form of (num_samples, timesteps,\n",
    "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
    "    # the first dimension to be the timesteps.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
    "# of the output sequence, decide which character should be chosen.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model each generation and show predictions against the validation\n",
    "# dataset.\n",
    "for iteration in range(1, 10):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=1,\n",
    "              validation_data=(x_val, y_val))\n",
    "    # Select 10 samples from the validation set at random so we can visualize\n",
    "    # errors.\n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2, 14, 15, 1, 16, 17, 18, 1, 3, 19, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "data = \"\"\" Jack and Jill went up the hill\\n\n",
    "\t\tTo fetch a pail of water\\n\n",
    "\t\tJack fell down and broke his crown\\n\n",
    "\t\tAnd Jill came tumbling after\\n \"\"\"\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "encoded = tokenizer.texts_to_sequences([data])[0]\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1]\n",
      "[1, 3]\n",
      "[3, 4]\n",
      "[4, 5]\n",
      "[5, 6]\n",
      "[6, 7]\n",
      "[7, 8]\n",
      "[8, 9]\n",
      "[9, 10]\n",
      "[10, 11]\n",
      "[11, 12]\n",
      "[12, 13]\n",
      "[13, 2]\n",
      "[2, 14]\n",
      "[14, 15]\n",
      "[15, 1]\n",
      "[1, 16]\n",
      "[16, 17]\n",
      "[17, 18]\n",
      "[18, 1]\n",
      "[1, 3]\n",
      "[3, 19]\n",
      "[19, 20]\n",
      "[20, 21]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i - 1 : i + 1]\n",
    "    print(sequence)\n",
    "    sequences.append(sequence)\n",
    "\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = array(sequences)\n",
    "x, y = sequences[:, 0], sequences[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 1, 10)             220       \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 50)                12200     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 22)                1122      \n",
      "=================================================================\n",
      "Total params: 13,542\n",
      "Trainable params: 13,542\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 1s - loss: 3.0907 - acc: 0.0417\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.0901 - acc: 0.0833\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.0894 - acc: 0.0833\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.0886 - acc: 0.0833\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.0878 - acc: 0.0833\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.0870 - acc: 0.1250\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.0862 - acc: 0.1250\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.0853 - acc: 0.1250\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.0845 - acc: 0.1250\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.0836 - acc: 0.1250\n",
      "Epoch 11/500\n",
      " - 0s - loss: 3.0828 - acc: 0.1250\n",
      "Epoch 12/500\n",
      " - 0s - loss: 3.0819 - acc: 0.1250\n",
      "Epoch 13/500\n",
      " - 0s - loss: 3.0810 - acc: 0.1250\n",
      "Epoch 14/500\n",
      " - 0s - loss: 3.0801 - acc: 0.1250\n",
      "Epoch 15/500\n",
      " - 0s - loss: 3.0792 - acc: 0.1250\n",
      "Epoch 16/500\n",
      " - 0s - loss: 3.0783 - acc: 0.1250\n",
      "Epoch 17/500\n",
      " - 0s - loss: 3.0773 - acc: 0.1250\n",
      "Epoch 18/500\n",
      " - 0s - loss: 3.0763 - acc: 0.1250\n",
      "Epoch 19/500\n",
      " - 0s - loss: 3.0753 - acc: 0.1250\n",
      "Epoch 20/500\n",
      " - 0s - loss: 3.0743 - acc: 0.1250\n",
      "Epoch 21/500\n",
      " - 0s - loss: 3.0733 - acc: 0.1250\n",
      "Epoch 22/500\n",
      " - 0s - loss: 3.0722 - acc: 0.1250\n",
      "Epoch 23/500\n",
      " - 0s - loss: 3.0711 - acc: 0.1250\n",
      "Epoch 24/500\n",
      " - 0s - loss: 3.0700 - acc: 0.1250\n",
      "Epoch 25/500\n",
      " - 0s - loss: 3.0689 - acc: 0.1250\n",
      "Epoch 26/500\n",
      " - 0s - loss: 3.0677 - acc: 0.1250\n",
      "Epoch 27/500\n",
      " - 0s - loss: 3.0665 - acc: 0.1250\n",
      "Epoch 28/500\n",
      " - 0s - loss: 3.0653 - acc: 0.1250\n",
      "Epoch 29/500\n",
      " - 0s - loss: 3.0640 - acc: 0.1250\n",
      "Epoch 30/500\n",
      " - 0s - loss: 3.0627 - acc: 0.1250\n",
      "Epoch 31/500\n",
      " - 0s - loss: 3.0614 - acc: 0.2083\n",
      "Epoch 32/500\n",
      " - 0s - loss: 3.0600 - acc: 0.2083\n",
      "Epoch 33/500\n",
      " - 0s - loss: 3.0586 - acc: 0.2083\n",
      "Epoch 34/500\n",
      " - 0s - loss: 3.0571 - acc: 0.2083\n",
      "Epoch 35/500\n",
      " - 0s - loss: 3.0556 - acc: 0.2083\n",
      "Epoch 36/500\n",
      " - 0s - loss: 3.0541 - acc: 0.2083\n",
      "Epoch 37/500\n",
      " - 0s - loss: 3.0525 - acc: 0.2083\n",
      "Epoch 38/500\n",
      " - 0s - loss: 3.0509 - acc: 0.2083\n",
      "Epoch 39/500\n",
      " - 0s - loss: 3.0492 - acc: 0.2083\n",
      "Epoch 40/500\n",
      " - 0s - loss: 3.0475 - acc: 0.2083\n",
      "Epoch 41/500\n",
      " - 0s - loss: 3.0458 - acc: 0.2083\n",
      "Epoch 42/500\n",
      " - 0s - loss: 3.0440 - acc: 0.2083\n",
      "Epoch 43/500\n",
      " - 0s - loss: 3.0421 - acc: 0.2083\n",
      "Epoch 44/500\n",
      " - 0s - loss: 3.0402 - acc: 0.2083\n",
      "Epoch 45/500\n",
      " - 0s - loss: 3.0382 - acc: 0.2083\n",
      "Epoch 46/500\n",
      " - 0s - loss: 3.0362 - acc: 0.2083\n",
      "Epoch 47/500\n",
      " - 0s - loss: 3.0341 - acc: 0.2083\n",
      "Epoch 48/500\n",
      " - 0s - loss: 3.0320 - acc: 0.2083\n",
      "Epoch 49/500\n",
      " - 0s - loss: 3.0297 - acc: 0.2083\n",
      "Epoch 50/500\n",
      " - 0s - loss: 3.0275 - acc: 0.2083\n",
      "Epoch 51/500\n",
      " - 0s - loss: 3.0252 - acc: 0.2083\n",
      "Epoch 52/500\n",
      " - 0s - loss: 3.0228 - acc: 0.2083\n",
      "Epoch 53/500\n",
      " - 0s - loss: 3.0203 - acc: 0.2083\n",
      "Epoch 54/500\n",
      " - 0s - loss: 3.0178 - acc: 0.2083\n",
      "Epoch 55/500\n",
      " - 0s - loss: 3.0152 - acc: 0.2083\n",
      "Epoch 56/500\n",
      " - 0s - loss: 3.0125 - acc: 0.2083\n",
      "Epoch 57/500\n",
      " - 0s - loss: 3.0098 - acc: 0.2083\n",
      "Epoch 58/500\n",
      " - 0s - loss: 3.0069 - acc: 0.2083\n",
      "Epoch 59/500\n",
      " - 0s - loss: 3.0040 - acc: 0.2083\n",
      "Epoch 60/500\n",
      " - 0s - loss: 3.0010 - acc: 0.2083\n",
      "Epoch 61/500\n",
      " - 0s - loss: 2.9979 - acc: 0.2083\n",
      "Epoch 62/500\n",
      " - 0s - loss: 2.9948 - acc: 0.2083\n",
      "Epoch 63/500\n",
      " - 0s - loss: 2.9915 - acc: 0.2083\n",
      "Epoch 64/500\n",
      " - 0s - loss: 2.9882 - acc: 0.2083\n",
      "Epoch 65/500\n",
      " - 0s - loss: 2.9848 - acc: 0.2083\n",
      "Epoch 66/500\n",
      " - 0s - loss: 2.9812 - acc: 0.2083\n",
      "Epoch 67/500\n",
      " - 0s - loss: 2.9776 - acc: 0.2083\n",
      "Epoch 68/500\n",
      " - 0s - loss: 2.9739 - acc: 0.2083\n",
      "Epoch 69/500\n",
      " - 0s - loss: 2.9701 - acc: 0.2083\n",
      "Epoch 70/500\n",
      " - 0s - loss: 2.9662 - acc: 0.2083\n",
      "Epoch 71/500\n",
      " - 0s - loss: 2.9621 - acc: 0.2083\n",
      "Epoch 72/500\n",
      " - 0s - loss: 2.9580 - acc: 0.2083\n",
      "Epoch 73/500\n",
      " - 0s - loss: 2.9538 - acc: 0.2083\n",
      "Epoch 74/500\n",
      " - 0s - loss: 2.9494 - acc: 0.2083\n",
      "Epoch 75/500\n",
      " - 0s - loss: 2.9450 - acc: 0.2083\n",
      "Epoch 76/500\n",
      " - 0s - loss: 2.9404 - acc: 0.2083\n",
      "Epoch 77/500\n",
      " - 0s - loss: 2.9357 - acc: 0.2083\n",
      "Epoch 78/500\n",
      " - 0s - loss: 2.9309 - acc: 0.2083\n",
      "Epoch 79/500\n",
      " - 0s - loss: 2.9260 - acc: 0.2083\n",
      "Epoch 80/500\n",
      " - 0s - loss: 2.9209 - acc: 0.2083\n",
      "Epoch 81/500\n",
      " - 0s - loss: 2.9157 - acc: 0.2083\n",
      "Epoch 82/500\n",
      " - 0s - loss: 2.9104 - acc: 0.2083\n",
      "Epoch 83/500\n",
      " - 0s - loss: 2.9050 - acc: 0.2083\n",
      "Epoch 84/500\n",
      " - 0s - loss: 2.8994 - acc: 0.2083\n",
      "Epoch 85/500\n",
      " - 0s - loss: 2.8937 - acc: 0.2083\n",
      "Epoch 86/500\n",
      " - 0s - loss: 2.8879 - acc: 0.2083\n",
      "Epoch 87/500\n",
      " - 0s - loss: 2.8819 - acc: 0.2083\n",
      "Epoch 88/500\n",
      " - 0s - loss: 2.8757 - acc: 0.2083\n",
      "Epoch 89/500\n",
      " - 0s - loss: 2.8695 - acc: 0.2083\n",
      "Epoch 90/500\n",
      " - 0s - loss: 2.8631 - acc: 0.2083\n",
      "Epoch 91/500\n",
      " - 0s - loss: 2.8565 - acc: 0.2083\n",
      "Epoch 92/500\n",
      " - 0s - loss: 2.8498 - acc: 0.2083\n",
      "Epoch 93/500\n",
      " - 0s - loss: 2.8430 - acc: 0.2083\n",
      "Epoch 94/500\n",
      " - 0s - loss: 2.8360 - acc: 0.2083\n",
      "Epoch 95/500\n",
      " - 0s - loss: 2.8288 - acc: 0.2083\n",
      "Epoch 96/500\n",
      " - 0s - loss: 2.8215 - acc: 0.2083\n",
      "Epoch 97/500\n",
      " - 0s - loss: 2.8140 - acc: 0.2083\n",
      "Epoch 98/500\n",
      " - 0s - loss: 2.8064 - acc: 0.2083\n",
      "Epoch 99/500\n",
      " - 0s - loss: 2.7986 - acc: 0.2083\n",
      "Epoch 100/500\n",
      " - 0s - loss: 2.7907 - acc: 0.2083\n",
      "Epoch 101/500\n",
      " - 0s - loss: 2.7826 - acc: 0.2083\n",
      "Epoch 102/500\n",
      " - 0s - loss: 2.7743 - acc: 0.2083\n",
      "Epoch 103/500\n",
      " - 0s - loss: 2.7659 - acc: 0.2083\n",
      "Epoch 104/500\n",
      " - 0s - loss: 2.7573 - acc: 0.2083\n",
      "Epoch 105/500\n",
      " - 0s - loss: 2.7485 - acc: 0.2083\n",
      "Epoch 106/500\n",
      " - 0s - loss: 2.7396 - acc: 0.2083\n",
      "Epoch 107/500\n",
      " - 0s - loss: 2.7305 - acc: 0.2083\n",
      "Epoch 108/500\n",
      " - 0s - loss: 2.7213 - acc: 0.2083\n",
      "Epoch 109/500\n",
      " - 0s - loss: 2.7119 - acc: 0.2083\n",
      "Epoch 110/500\n",
      " - 0s - loss: 2.7023 - acc: 0.2083\n",
      "Epoch 111/500\n",
      " - 0s - loss: 2.6926 - acc: 0.2083\n",
      "Epoch 112/500\n",
      " - 0s - loss: 2.6826 - acc: 0.2083\n",
      "Epoch 113/500\n",
      " - 0s - loss: 2.6725 - acc: 0.2083\n",
      "Epoch 114/500\n",
      " - 0s - loss: 2.6623 - acc: 0.2083\n",
      "Epoch 115/500\n",
      " - 0s - loss: 2.6519 - acc: 0.2083\n",
      "Epoch 116/500\n",
      " - 0s - loss: 2.6413 - acc: 0.2083\n",
      "Epoch 117/500\n",
      " - 0s - loss: 2.6305 - acc: 0.2083\n",
      "Epoch 118/500\n",
      " - 0s - loss: 2.6196 - acc: 0.2083\n",
      "Epoch 119/500\n",
      " - 0s - loss: 2.6086 - acc: 0.2083\n",
      "Epoch 120/500\n",
      " - 0s - loss: 2.5973 - acc: 0.2083\n",
      "Epoch 121/500\n",
      " - 0s - loss: 2.5859 - acc: 0.2083\n",
      "Epoch 122/500\n",
      " - 0s - loss: 2.5744 - acc: 0.2083\n",
      "Epoch 123/500\n",
      " - 0s - loss: 2.5627 - acc: 0.2083\n",
      "Epoch 124/500\n",
      " - 0s - loss: 2.5509 - acc: 0.2500\n",
      "Epoch 125/500\n",
      " - 0s - loss: 2.5389 - acc: 0.2500\n",
      "Epoch 126/500\n",
      " - 0s - loss: 2.5268 - acc: 0.2500\n",
      "Epoch 127/500\n",
      " - 0s - loss: 2.5146 - acc: 0.2500\n",
      "Epoch 128/500\n",
      " - 0s - loss: 2.5021 - acc: 0.2500\n",
      "Epoch 129/500\n",
      " - 0s - loss: 2.4896 - acc: 0.2500\n",
      "Epoch 130/500\n",
      " - 0s - loss: 2.4769 - acc: 0.2500\n",
      "Epoch 131/500\n",
      " - 0s - loss: 2.4641 - acc: 0.2500\n",
      "Epoch 132/500\n",
      " - 0s - loss: 2.4511 - acc: 0.2917\n",
      "Epoch 133/500\n",
      " - 0s - loss: 2.4380 - acc: 0.3333\n",
      "Epoch 134/500\n",
      " - 0s - loss: 2.4248 - acc: 0.3333\n",
      "Epoch 135/500\n",
      " - 0s - loss: 2.4116 - acc: 0.3333\n",
      "Epoch 136/500\n",
      " - 0s - loss: 2.3981 - acc: 0.3333\n",
      "Epoch 137/500\n",
      " - 0s - loss: 2.3846 - acc: 0.3333\n",
      "Epoch 138/500\n",
      " - 0s - loss: 2.3709 - acc: 0.3333\n",
      "Epoch 139/500\n",
      " - 0s - loss: 2.3571 - acc: 0.3333\n",
      "Epoch 140/500\n",
      " - 0s - loss: 2.3433 - acc: 0.3333\n",
      "Epoch 141/500\n",
      " - 0s - loss: 2.3294 - acc: 0.3333\n",
      "Epoch 142/500\n",
      " - 0s - loss: 2.3153 - acc: 0.3750\n",
      "Epoch 143/500\n",
      " - 0s - loss: 2.3012 - acc: 0.3750\n",
      "Epoch 144/500\n",
      " - 0s - loss: 2.2869 - acc: 0.3750\n",
      "Epoch 145/500\n",
      " - 0s - loss: 2.2727 - acc: 0.3750\n",
      "Epoch 146/500\n",
      " - 0s - loss: 2.2583 - acc: 0.3750\n",
      "Epoch 147/500\n",
      " - 0s - loss: 2.2439 - acc: 0.3750\n",
      "Epoch 148/500\n",
      " - 0s - loss: 2.2293 - acc: 0.4167\n",
      "Epoch 149/500\n",
      " - 0s - loss: 2.2148 - acc: 0.4167\n",
      "Epoch 150/500\n",
      " - 0s - loss: 2.2002 - acc: 0.4167\n",
      "Epoch 151/500\n",
      " - 0s - loss: 2.1855 - acc: 0.4167\n",
      "Epoch 152/500\n",
      " - 0s - loss: 2.1707 - acc: 0.4167\n",
      "Epoch 153/500\n",
      " - 0s - loss: 2.1559 - acc: 0.4167\n",
      "Epoch 154/500\n",
      " - 0s - loss: 2.1411 - acc: 0.4167\n",
      "Epoch 155/500\n",
      " - 0s - loss: 2.1262 - acc: 0.4167\n",
      "Epoch 156/500\n",
      " - 0s - loss: 2.1112 - acc: 0.4167\n",
      "Epoch 157/500\n",
      " - 0s - loss: 2.0963 - acc: 0.4167\n",
      "Epoch 158/500\n",
      " - 0s - loss: 2.0813 - acc: 0.4167\n",
      "Epoch 159/500\n",
      " - 0s - loss: 2.0662 - acc: 0.4167\n",
      "Epoch 160/500\n",
      " - 0s - loss: 2.0512 - acc: 0.4167\n",
      "Epoch 161/500\n",
      " - 0s - loss: 2.0361 - acc: 0.5000\n",
      "Epoch 162/500\n",
      " - 0s - loss: 2.0210 - acc: 0.5000\n",
      "Epoch 163/500\n",
      " - 0s - loss: 2.0058 - acc: 0.5000\n",
      "Epoch 164/500\n",
      " - 0s - loss: 1.9907 - acc: 0.5000\n",
      "Epoch 165/500\n",
      " - 0s - loss: 1.9755 - acc: 0.5000\n",
      "Epoch 166/500\n",
      " - 0s - loss: 1.9603 - acc: 0.5000\n",
      "Epoch 167/500\n",
      " - 0s - loss: 1.9452 - acc: 0.5000\n",
      "Epoch 168/500\n",
      " - 0s - loss: 1.9300 - acc: 0.5000\n",
      "Epoch 169/500\n",
      " - 0s - loss: 1.9148 - acc: 0.5000\n",
      "Epoch 170/500\n",
      " - 0s - loss: 1.8996 - acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/500\n",
      " - 0s - loss: 1.8844 - acc: 0.5000\n",
      "Epoch 172/500\n",
      " - 0s - loss: 1.8692 - acc: 0.5417\n",
      "Epoch 173/500\n",
      " - 0s - loss: 1.8540 - acc: 0.5417\n",
      "Epoch 174/500\n",
      " - 0s - loss: 1.8388 - acc: 0.5417\n",
      "Epoch 175/500\n",
      " - 0s - loss: 1.8236 - acc: 0.5417\n",
      "Epoch 176/500\n",
      " - 0s - loss: 1.8084 - acc: 0.5417\n",
      "Epoch 177/500\n",
      " - 0s - loss: 1.7932 - acc: 0.5417\n",
      "Epoch 178/500\n",
      " - 0s - loss: 1.7780 - acc: 0.5417\n",
      "Epoch 179/500\n",
      " - 0s - loss: 1.7629 - acc: 0.5417\n",
      "Epoch 180/500\n",
      " - 0s - loss: 1.7477 - acc: 0.5417\n",
      "Epoch 181/500\n",
      " - 0s - loss: 1.7326 - acc: 0.5833\n",
      "Epoch 182/500\n",
      " - 0s - loss: 1.7175 - acc: 0.5833\n",
      "Epoch 183/500\n",
      " - 0s - loss: 1.7024 - acc: 0.6250\n",
      "Epoch 184/500\n",
      " - 0s - loss: 1.6873 - acc: 0.6250\n",
      "Epoch 185/500\n",
      " - 0s - loss: 1.6723 - acc: 0.6250\n",
      "Epoch 186/500\n",
      " - 0s - loss: 1.6573 - acc: 0.6250\n",
      "Epoch 187/500\n",
      " - 0s - loss: 1.6423 - acc: 0.7083\n",
      "Epoch 188/500\n",
      " - 0s - loss: 1.6273 - acc: 0.7083\n",
      "Epoch 189/500\n",
      " - 0s - loss: 1.6124 - acc: 0.7083\n",
      "Epoch 190/500\n",
      " - 0s - loss: 1.5974 - acc: 0.7500\n",
      "Epoch 191/500\n",
      " - 0s - loss: 1.5826 - acc: 0.7917\n",
      "Epoch 192/500\n",
      " - 0s - loss: 1.5677 - acc: 0.7917\n",
      "Epoch 193/500\n",
      " - 0s - loss: 1.5529 - acc: 0.7917\n",
      "Epoch 194/500\n",
      " - 0s - loss: 1.5382 - acc: 0.7917\n",
      "Epoch 195/500\n",
      " - 0s - loss: 1.5235 - acc: 0.7917\n",
      "Epoch 196/500\n",
      " - 0s - loss: 1.5088 - acc: 0.8333\n",
      "Epoch 197/500\n",
      " - 0s - loss: 1.4942 - acc: 0.8333\n",
      "Epoch 198/500\n",
      " - 0s - loss: 1.4796 - acc: 0.8333\n",
      "Epoch 199/500\n",
      " - 0s - loss: 1.4651 - acc: 0.8333\n",
      "Epoch 200/500\n",
      " - 0s - loss: 1.4506 - acc: 0.8333\n",
      "Epoch 201/500\n",
      " - 0s - loss: 1.4362 - acc: 0.8333\n",
      "Epoch 202/500\n",
      " - 0s - loss: 1.4219 - acc: 0.8333\n",
      "Epoch 203/500\n",
      " - 0s - loss: 1.4076 - acc: 0.8333\n",
      "Epoch 204/500\n",
      " - 0s - loss: 1.3933 - acc: 0.8333\n",
      "Epoch 205/500\n",
      " - 0s - loss: 1.3791 - acc: 0.8333\n",
      "Epoch 206/500\n",
      " - 0s - loss: 1.3650 - acc: 0.8333\n",
      "Epoch 207/500\n",
      " - 0s - loss: 1.3510 - acc: 0.8333\n",
      "Epoch 208/500\n",
      " - 0s - loss: 1.3370 - acc: 0.8750\n",
      "Epoch 209/500\n",
      " - 0s - loss: 1.3231 - acc: 0.8750\n",
      "Epoch 210/500\n",
      " - 0s - loss: 1.3092 - acc: 0.8750\n",
      "Epoch 211/500\n",
      " - 0s - loss: 1.2954 - acc: 0.8750\n",
      "Epoch 212/500\n",
      " - 0s - loss: 1.2818 - acc: 0.8750\n",
      "Epoch 213/500\n",
      " - 0s - loss: 1.2682 - acc: 0.8750\n",
      "Epoch 214/500\n",
      " - 0s - loss: 1.2546 - acc: 0.8750\n",
      "Epoch 215/500\n",
      " - 0s - loss: 1.2412 - acc: 0.8750\n",
      "Epoch 216/500\n",
      " - 0s - loss: 1.2278 - acc: 0.8750\n",
      "Epoch 217/500\n",
      " - 0s - loss: 1.2145 - acc: 0.8750\n",
      "Epoch 218/500\n",
      " - 0s - loss: 1.2013 - acc: 0.8750\n",
      "Epoch 219/500\n",
      " - 0s - loss: 1.1882 - acc: 0.8750\n",
      "Epoch 220/500\n",
      " - 0s - loss: 1.1751 - acc: 0.8750\n",
      "Epoch 221/500\n",
      " - 0s - loss: 1.1622 - acc: 0.8750\n",
      "Epoch 222/500\n",
      " - 0s - loss: 1.1493 - acc: 0.8750\n",
      "Epoch 223/500\n",
      " - 0s - loss: 1.1366 - acc: 0.8750\n",
      "Epoch 224/500\n",
      " - 0s - loss: 1.1239 - acc: 0.8750\n",
      "Epoch 225/500\n",
      " - 0s - loss: 1.1114 - acc: 0.8750\n",
      "Epoch 226/500\n",
      " - 0s - loss: 1.0989 - acc: 0.8750\n",
      "Epoch 227/500\n",
      " - 0s - loss: 1.0865 - acc: 0.8750\n",
      "Epoch 228/500\n",
      " - 0s - loss: 1.0743 - acc: 0.8750\n",
      "Epoch 229/500\n",
      " - 0s - loss: 1.0621 - acc: 0.8750\n",
      "Epoch 230/500\n",
      " - 0s - loss: 1.0500 - acc: 0.8750\n",
      "Epoch 231/500\n",
      " - 0s - loss: 1.0381 - acc: 0.8750\n",
      "Epoch 232/500\n",
      " - 0s - loss: 1.0262 - acc: 0.8750\n",
      "Epoch 233/500\n",
      " - 0s - loss: 1.0145 - acc: 0.8750\n",
      "Epoch 234/500\n",
      " - 0s - loss: 1.0028 - acc: 0.8750\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.9913 - acc: 0.8750\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.9798 - acc: 0.8750\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.9685 - acc: 0.8750\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.9573 - acc: 0.8750\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.9462 - acc: 0.8750\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.9352 - acc: 0.8750\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.9243 - acc: 0.8750\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.9135 - acc: 0.8750\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.9028 - acc: 0.8750\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.8923 - acc: 0.8750\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.8819 - acc: 0.8750\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.8716 - acc: 0.8750\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.8613 - acc: 0.8750\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.8513 - acc: 0.8750\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.8413 - acc: 0.8750\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.8314 - acc: 0.8750\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.8217 - acc: 0.8750\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.8121 - acc: 0.8750\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.8026 - acc: 0.8750\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.7932 - acc: 0.8750\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.7839 - acc: 0.8750\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.7747 - acc: 0.8750\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.7657 - acc: 0.8750\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.7568 - acc: 0.8750\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.7480 - acc: 0.8750\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.7393 - acc: 0.8750\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.7307 - acc: 0.8750\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.7222 - acc: 0.8750\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.7139 - acc: 0.8750\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.7057 - acc: 0.8750\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.6976 - acc: 0.8750\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.6896 - acc: 0.8750\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.6817 - acc: 0.8750\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.6739 - acc: 0.8750\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.6663 - acc: 0.8750\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.6587 - acc: 0.8750\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.6513 - acc: 0.8750\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.6440 - acc: 0.8750\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.6368 - acc: 0.8750\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.6297 - acc: 0.8750\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.6227 - acc: 0.8750\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.6158 - acc: 0.8750\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.6090 - acc: 0.8750\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.6024 - acc: 0.8750\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.5958 - acc: 0.8750\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.5894 - acc: 0.8750\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.5830 - acc: 0.8750\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.5768 - acc: 0.8750\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.5706 - acc: 0.8750\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.5646 - acc: 0.8750\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.5586 - acc: 0.8750\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.5528 - acc: 0.8750\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.5471 - acc: 0.8750\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.5414 - acc: 0.8750\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.5359 - acc: 0.8750\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.5304 - acc: 0.8750\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.5251 - acc: 0.8750\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.5198 - acc: 0.8750\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.5146 - acc: 0.8750\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.5095 - acc: 0.8750\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.5045 - acc: 0.8750\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.4997 - acc: 0.8750\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.4948 - acc: 0.8750\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.4901 - acc: 0.8750\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.4855 - acc: 0.8750\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.4809 - acc: 0.8750\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.4764 - acc: 0.8750\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.4720 - acc: 0.8750\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.4677 - acc: 0.8750\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.4635 - acc: 0.8750\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.4593 - acc: 0.8750\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.4553 - acc: 0.8750\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.4513 - acc: 0.8750\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.4473 - acc: 0.8750\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.4435 - acc: 0.8750\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.4397 - acc: 0.8750\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.4360 - acc: 0.8750\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.4324 - acc: 0.8750\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.4288 - acc: 0.8750\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.4253 - acc: 0.8750\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.4218 - acc: 0.8750\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.4184 - acc: 0.8750\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.4151 - acc: 0.8750\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.4119 - acc: 0.8750\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.4087 - acc: 0.8750\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.4055 - acc: 0.8750\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.4024 - acc: 0.8750\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.3994 - acc: 0.8750\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.3965 - acc: 0.8750\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.3935 - acc: 0.8750\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.3907 - acc: 0.8750\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.3879 - acc: 0.8750\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.3851 - acc: 0.8750\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.3824 - acc: 0.8750\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.3798 - acc: 0.8750\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.3772 - acc: 0.8750\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.3746 - acc: 0.8750\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.3721 - acc: 0.8750\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.3697 - acc: 0.8750\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.3672 - acc: 0.8750\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.3649 - acc: 0.8750\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.3625 - acc: 0.8750\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.3603 - acc: 0.8750\n",
      "Epoch 338/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 0.3580 - acc: 0.8750\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.3558 - acc: 0.8750\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.3537 - acc: 0.8750\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.3516 - acc: 0.8750\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.3495 - acc: 0.8750\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.3474 - acc: 0.8750\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.3454 - acc: 0.8750\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.3435 - acc: 0.8750\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.3415 - acc: 0.8750\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.3396 - acc: 0.8750\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.3378 - acc: 0.8750\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.3359 - acc: 0.8750\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.3341 - acc: 0.8750\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.3324 - acc: 0.8750\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.3306 - acc: 0.8750\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.3289 - acc: 0.8750\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.3272 - acc: 0.8750\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.3256 - acc: 0.8750\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.3240 - acc: 0.8750\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.3224 - acc: 0.8750\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.3208 - acc: 0.8750\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.3193 - acc: 0.8750\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.3177 - acc: 0.8750\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.3163 - acc: 0.8750\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.3148 - acc: 0.8750\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.3134 - acc: 0.8750\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.3120 - acc: 0.8750\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.3106 - acc: 0.8750\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.3092 - acc: 0.8750\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.3079 - acc: 0.8750\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.3065 - acc: 0.8750\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.3052 - acc: 0.8750\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.3040 - acc: 0.8750\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.3027 - acc: 0.8750\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.3015 - acc: 0.8750\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.3003 - acc: 0.8750\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.2991 - acc: 0.8750\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.2979 - acc: 0.8750\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.2968 - acc: 0.8750\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.2956 - acc: 0.8750\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.2945 - acc: 0.8750\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.2934 - acc: 0.8750\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.2923 - acc: 0.8750\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.2913 - acc: 0.8750\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.2902 - acc: 0.8750\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.2892 - acc: 0.8750\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.2882 - acc: 0.8750\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.2872 - acc: 0.8750\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.2862 - acc: 0.8750\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.2852 - acc: 0.8750\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.2843 - acc: 0.8750\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.2833 - acc: 0.8750\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.2824 - acc: 0.8750\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.2815 - acc: 0.8750\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.2806 - acc: 0.8750\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.2797 - acc: 0.8750\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.2789 - acc: 0.8750\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.2780 - acc: 0.8750\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.2772 - acc: 0.8750\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.2763 - acc: 0.8750\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.2755 - acc: 0.8750\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.2747 - acc: 0.8750\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.2739 - acc: 0.8750\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.2731 - acc: 0.8750\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.2724 - acc: 0.8750\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.2716 - acc: 0.8750\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.2709 - acc: 0.8750\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.2701 - acc: 0.8750\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.2694 - acc: 0.8750\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.2687 - acc: 0.8750\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.2680 - acc: 0.8750\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.2673 - acc: 0.8750\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.2666 - acc: 0.8750\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.2660 - acc: 0.8750\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.2653 - acc: 0.8750\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.2647 - acc: 0.8750\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.2640 - acc: 0.8750\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.2634 - acc: 0.8750\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.2628 - acc: 0.8750\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.2621 - acc: 0.8750\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.2615 - acc: 0.8750\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.2609 - acc: 0.8750\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.2603 - acc: 0.8750\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.2598 - acc: 0.8750\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.2592 - acc: 0.8750\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.2586 - acc: 0.8750\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.2581 - acc: 0.8750\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.2575 - acc: 0.8750\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.2570 - acc: 0.8750\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.2564 - acc: 0.8750\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.2559 - acc: 0.8750\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.2554 - acc: 0.8750\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.2549 - acc: 0.8750\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.2543 - acc: 0.8750\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.2538 - acc: 0.8750\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.2533 - acc: 0.8750\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.2529 - acc: 0.8750\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.2524 - acc: 0.8750\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.2519 - acc: 0.8750\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.2514 - acc: 0.8750\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.2510 - acc: 0.8750\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.2505 - acc: 0.8750\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.2500 - acc: 0.8750\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.2496 - acc: 0.8750\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.2491 - acc: 0.8750\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.2487 - acc: 0.8750\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.2483 - acc: 0.8750\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.2479 - acc: 0.8750\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.2474 - acc: 0.8750\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.2470 - acc: 0.8750\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.2466 - acc: 0.8750\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.2462 - acc: 0.8750\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.2458 - acc: 0.8750\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.2454 - acc: 0.8750\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.2450 - acc: 0.8750\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.2446 - acc: 0.8750\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.2442 - acc: 0.8750\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.2439 - acc: 0.8750\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.2435 - acc: 0.8750\n",
      "Epoch 457/500\n",
      " - 0s - loss: 0.2431 - acc: 0.8750\n",
      "Epoch 458/500\n",
      " - 0s - loss: 0.2428 - acc: 0.8750\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.2424 - acc: 0.8750\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.2420 - acc: 0.8750\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.2417 - acc: 0.8750\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.2413 - acc: 0.8750\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.2410 - acc: 0.8750\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.2406 - acc: 0.8750\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.2403 - acc: 0.8750\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.2400 - acc: 0.8750\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.2396 - acc: 0.8750\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.2393 - acc: 0.8750\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.2390 - acc: 0.8750\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.2387 - acc: 0.8750\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.2384 - acc: 0.8750\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.2381 - acc: 0.8750\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.2377 - acc: 0.8750\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.2374 - acc: 0.8750\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.2371 - acc: 0.8750\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.2368 - acc: 0.8750\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.2365 - acc: 0.8750\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.2363 - acc: 0.8750\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.2360 - acc: 0.8750\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.2357 - acc: 0.8750\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.2354 - acc: 0.8750\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.2351 - acc: 0.8750\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.2348 - acc: 0.8750\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.2346 - acc: 0.8750\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.2343 - acc: 0.8750\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.2340 - acc: 0.8750\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.2338 - acc: 0.8750\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.2335 - acc: 0.8750\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.2332 - acc: 0.8750\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.2330 - acc: 0.8750\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.2327 - acc: 0.8750\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.2325 - acc: 0.8750\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.2322 - acc: 0.8750\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.2320 - acc: 0.8750\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.2317 - acc: 0.8750\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.2315 - acc: 0.8750\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.2313 - acc: 0.8750\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.2310 - acc: 0.8750\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.2308 - acc: 0.8750\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.2306 - acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f247bfd9f28>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x, y, epochs=500, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water\n",
      "jack\n"
     ]
    }
   ],
   "source": [
    "in_text = 'water'\n",
    "print(in_text)\n",
    "encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "encoded = array(encoded)\n",
    "yhat = model.predict_classes(encoded, verbose=0)\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
